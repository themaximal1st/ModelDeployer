<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="description" content="Model Deployer lets you easily deploy, rate limit, and manage AI models in production" />
  <title>Model Deployer — Deploy AI models in production</title>
  <style>
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
    .display.math{display: block; text-align: center; margin: 0.5rem auto;}
    /* CSS for syntax highlighting */
    pre > code.sourceCode { white-space: pre; position: relative; }
    pre > code.sourceCode > span { line-height: 1.25; }
    pre > code.sourceCode > span:empty { height: 1.2em; }
    .sourceCode { overflow: visible; }
    code.sourceCode > span { color: inherit; text-decoration: inherit; }
    div.sourceCode { margin: 1em 0; }
    pre.sourceCode { margin: 0; }
    @media screen {
    div.sourceCode { overflow: auto; }
    }
    @media print {
    pre > code.sourceCode { white-space: pre-wrap; }
    pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
    }
    pre.numberSource code
      { counter-reset: source-line 0; }
    pre.numberSource code > span
      { position: relative; left: -4em; counter-increment: source-line; }
    pre.numberSource code > span > a:first-child::before
      { content: counter(source-line);
        position: relative; left: -1em; text-align: right; vertical-align: baseline;
        border: none; display: inline-block;
        -webkit-touch-callout: none; -webkit-user-select: none;
        -khtml-user-select: none; -moz-user-select: none;
        -ms-user-select: none; user-select: none;
        padding: 0 4px; width: 4em;
        background-color: #2a211c;
        color: #bdae9d;
      }
    pre.numberSource { margin-left: 3em; border-left: 1px solid #bdae9d;  padding-left: 4px; }
    div.sourceCode
      { color: #bdae9d; background-color: #2a211c; }
    @media screen {
    pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
    }
    code span.al { color: #ffff00; } /* Alert */
    code span.an { color: #0066ff; font-weight: bold; font-style: italic; } /* Annotation */
    code span.at { } /* Attribute */
    code span.bn { color: #44aa43; } /* BaseN */
    code span.bu { } /* BuiltIn */
    code span.cf { color: #43a8ed; font-weight: bold; } /* ControlFlow */
    code span.ch { color: #049b0a; } /* Char */
    code span.cn { } /* Constant */
    code span.co { color: #888888; font-style: italic; } /* Comment */
    code span.do { color: #0066ff; font-style: italic; } /* Documentation */
    code span.dt { text-decoration: underline; } /* DataType */
    code span.dv { color: #44aa43; } /* DecVal */
    code span.er { color: #ffff00; font-weight: bold; } /* Error */
    code span.ex { } /* Extension */
    code span.fl { color: #44aa43; } /* Float */
    code span.fu { color: #ff9358; font-weight: bold; } /* Function */
    code span.im { } /* Import */
    code span.in { color: #0066ff; font-weight: bold; font-style: italic; } /* Information */
    code span.kw { color: #43a8ed; font-weight: bold; } /* Keyword */
    code span.op { } /* Operator */
    code span.pp { font-weight: bold; } /* Preprocessor */
    code span.sc { color: #049b0a; } /* SpecialChar */
    code span.ss { color: #049b0a; } /* SpecialString */
    code span.st { color: #049b0a; } /* String */
    code span.va { } /* Variable */
    code span.vs { color: #049b0a; } /* VerbatimString */
    code span.wa { color: #ffff00; font-weight: bold; } /* Warning */
  </style>
  <link rel="stylesheet" href="style.css" />
  <script defer data-domain="modeldeployer.themaximalist.com" src="https://s.cac.app/js/script.outbound-links.js"></script>
</head>
<body>
<a class="fork-me" href="https://github.com/themaximal1st/modeldeployer"><img decoding="async" width="149" height="149" src="https://github.blog/wp-content/uploads/2008/12/forkme_right_darkblue_121621.png?resize=149%2C149" class="attachment-full size-full" alt="Fork me on GitHub" loading="lazy" data-recalc-dims="1"></a>
<header id="title-block-header">
<h1 class="title">Model Deployer — Deploy AI models in production</h1>
</header>
<h1 id="model-deployer">Model Deployer</h1>
<p><img src="logo.png" /></p>
<div class="badges" style="text-align: center; margin-top: 0px;">
<p><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/themaximal1st/modeldeployer">
<img alt="NPM Downloads" src="https://img.shields.io/npm/dt/%40themaximalist%2Fmodeldeployer">
<img alt="GitHub code size in bytes" src="https://img.shields.io/github/languages/code-size/themaximal1st/modeldeployer">
<img alt="GitHub License" src="https://img.shields.io/github/license/themaximal1st/modeldeployer"></p>
</div>
<p><br /></p>
<p><a href="https://modeldeployer.com">Model Deployer</a> is the
simplest way to deploy AI models for your applications.</p>
<p><strong>Features:</strong></p>
<ul>
<li>Proxy to hundreds of local and remote AI models (LLM, Stable
Diffusion, VectorDBs)</li>
<li>A single interface for all LLM models, built on <a
href="https://llmjs.themaximalist.com">LLM.js</a></li>
<li>A single interface for all embeddings, built on <a
href="https://embeddingsjs.themaximalist.com">Embeddings.js</a></li>
<li>Easily view usage history for each app user</li>
<li>Rate-limit users based on pre-defined limits</li>
<li>Track cost and usage cross hundreds of AI models</li>
<li>Free, paid, and user-provided API key are supported</li>
<li>Prevent free users from consuming your API tokens</li>
<li>Pre-pay with credits <em>(coming soon)</em></li>
<li>Monthly subscription <em>(coming soon)</em></li>
<li>Self-hosted or hosted <a
href="https://modeldeployer.com">modeldeployer.com</a></li>
<li>MIT License</li>
</ul>
<h2 id="getting-started">Getting started</h2>
<p>You’ll need a Postgres DB setup. Then in your shell:</p>
<div class="sourceCode" id="cb1"><pre
class="sourceCode bash"><code class="sourceCode bash"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="ex">npm</span> install @themaximalist/modeldeployer</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="bu">cd</span> modeldeployer</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="ex">npm</span> install</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="fu">cp</span> .env.template .env <span class="co"># edit DB connection</span></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="ex">npm</span> run dev</span></code></pre></div>
<p>Visit <a href="http://localhost:3000">http://localhost:3000</a> and
you’ll see Model Deployer.</p>
<p>You should be able to create a user, a model, and an API key.</p>
<p>Then you can use <code>LLM.js</code> to interact with your API:</p>
<div class="sourceCode" id="cb2"><pre
class="sourceCode javascript"><code class="sourceCode javascript"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> LLM <span class="im">from</span> <span class="st">&quot;@themaximalist/llm.js&quot;</span></span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a><span class="cf">await</span> <span class="fu">LLM</span>(<span class="st">&quot;what color is the sky?&quot;</span><span class="op">,</span> { <span class="dt">service</span><span class="op">:</span> <span class="st">&quot;modeldeployer&quot;</span><span class="op">,</span> <span class="dt">model</span><span class="op">:</span> <span class="st">&quot;model-api-key-goes-here&quot;</span> })<span class="op">;</span></span></code></pre></div>
<h2 id="embeddings">Embeddings</h2>
<p>Model Deployer also works with <code>embeddings.js</code>:</p>
<div class="sourceCode" id="cb3"><pre
class="sourceCode javascript"><code class="sourceCode javascript"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> Embeddings <span class="im">from</span> <span class="st">&quot;@themaximalist/embeddings.js&quot;</span></span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a><span class="cf">await</span> <span class="fu">Embeddings</span>(<span class="st">&quot;what color is the sky?&quot;</span><span class="op">,</span> { <span class="dt">service</span><span class="op">:</span> <span class="st">&quot;modeldeployer&quot;</span><span class="op">,</span> <span class="dt">model</span><span class="op">:</span> <span class="st">&quot;embeddings-api-key-goes-here&quot;</span> })<span class="op">;</span></span></code></pre></div>
<p>More documentation coming soon!</p>
<h2 id="why-does-model-deployer-exist">Why does Model Deployer
exist?</h2>
<p>I built <a href="https://llmjs.themaximalist.com">LLM.js</a> because
I wanted to give people control over how they use AI models in their
apps.</p>
<p>It’s great to use GPT-4 and Claude, but it sucks to get locked in.
And it’s hard to use local models.</p>
<p><code>LLM.js</code> solves these problems, by creating a single
simple interface that works with dozens of popular models.</p>
<p>As great as it is, it doesn’t fully solve the problem.</p>
<p>Bundling an app with a local model is not practical, the binaries are
hundreds of MBs or even GBs.</p>
<p>Downloading the model on first start also isn’t practical. Some users
will patiently sit through this, but most won’t. The first 10 seconds
mean everything on a new app—making users wait will not work.</p>
<p><strong>How do you offer the power of server models, with the
flexibility of local open-source models?</strong></p>
<p>Model Deployer is the solution. It’s an open-source server to manage
models for your users. It trades a little bit of privacy for user
experience.</p>
<p>Importantly, it’s built on a 100% open stack. So for users who care,
(if you’re reading this you probably care), there are ways to go fully
local, and self hosted.</p>
<p>This accomplishes the best of both worlds. Free, open-source, MIT
licensed model deployment tech that integrates into the existing
remote/local AI toolchain.</p>
<h2 id="author">Author</h2>
<ul>
<li><a href="https://themaximalist.com/">The Maximalist</a></li>
<li><a href="https://twitter.com/themaximal1st"><span class="citation"
data-cites="themaximal1st">@themaximal1st</span></a></li>
</ul>
<h2 id="license">License</h2>
<p>MIT</p>
</body>
</html>
